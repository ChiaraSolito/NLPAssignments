{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import webtext\n",
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRANYATAN', 'UMUM', 'NGENANI', 'HAK', '-', 'HAK', ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = nltk.corpus.gutenberg\n",
    "texts = nltk.corpus.webtext\n",
    "spanish_cess = nltk.corpus.cess_esp\n",
    "nltk.corpus.udhr.fileids()\n",
    "nltk.corpus.udhr.words('Javanese-Latin1')\n",
    "\n",
    "\n",
    "#data = IO CREATE A DATASET THAT ASSOCIATE TO THIS CORPUS A LABEL \"english - 1\" AND \"nonenglish - 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cose da fare:\n",
    "1. Fare in modo che si possa dare un input oltre a quello già presente, fatto come si vuole\n",
    "2. Modificare il codice piano piano\n",
    "3. Timeline:\n",
    "    - da consegnare entro il 30 nov.\n",
    "    - prima implementazione entro il 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Tags\n",
    "Since the dataset presents both html tags, urls and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(string):\n",
    "    removelist = \"\"\n",
    "    result = re.sub('','',string)          #remove HTML tags\n",
    "    result = re.sub('https://.*','',result)   #remove URLs\n",
    "    result = re.sub(r'[^w'+removelist+']', ' ',result)    #remove non-alphanumeric characters \n",
    "    result = result.lower()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review']=data['review'].apply(lambda cw : remove_tags(cw)) \n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['review'] = data['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates tokenizer, lematizer and lematize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    st = \"\"\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        st = st + lemmatizer.lemmatize(w) + \" \"\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data.review.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset from its labels\n",
    "LabelEncoder() from sklearn.preprocessing is used to convert the labels (‘positive’, ‘negative’) into 1’s and 0’s respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = data['review'].values\n",
    "labels = data['sentiment'].values\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, stratify = encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier\n",
    "We are going to be building our own classifier from scratch using the formulas described earlier. We start by using the CountVectorizer from sklearn.feature_extraction.text to get the frequency of each word appearing in the training set. We store them in a dictionary called ‘word_counts’. All the unique words in the corpus are stored in ‘vocab’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiara/.conda/envs/nlp-lab/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(max_features = 3000)\n",
    "X = vec.fit_transform(train_sentences)\n",
    "vocab = vec.get_feature_names()\n",
    "X = X.toarray()\n",
    "word_counts = {}\n",
    "for l in range(2):\n",
    "    word_counts[l] = defaultdict(lambda: 0)\n",
    "for i in range(X.shape[0]):\n",
    "    l = train_labels[i]\n",
    "    for j in range(len(vocab)):\n",
    "        word_counts[l][vocab[j]] += X[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(n_label_items, vocab, word_counts, word, text_label):\n",
    "    a = word_counts[text_label][word] + 1\n",
    "    b = n_label_items[text_label] + len(vocab)\n",
    "    return math.log(a/b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining fit and predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_label(x, y, labels):\n",
    "    data = {}\n",
    "    for l in labels:\n",
    "        data[l] = x[np.where(y == l)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x, y, labels):\n",
    "    n_label_items = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(x)\n",
    "    grouped_data = group_by_label(x, y, labels)\n",
    "    for l, data in grouped_data.items():\n",
    "        n_label_items[l] = len(data)\n",
    "        log_label_priors[l] = math.log(n_label_items[l] / n)\n",
    "    return n_label_items, log_label_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = {l: log_label_priors[l] for l in labels}\n",
    "        words = set(w_tokenizer.tokenize(text))\n",
    "        for word in words:\n",
    "            if word not in vocab: continue\n",
    "            for l in labels:\n",
    "                log_w_given_l = laplace_smoothing(n_label_items, vocab, word_counts, word, l)\n",
    "                label_scores[l] += log_w_given_l\n",
    "        result.append(max(label_scores, key=label_scores.get))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model on Training Set and Evaluating Accuracies on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction on test set :  0.49992\n"
     ]
    }
   ],
   "source": [
    "labels = [0,1]\n",
    "n_label_items, log_label_priors = fit(train_sentences,train_labels,labels)\n",
    "pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_sentences)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(test_labels,pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp-lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c64eeec08d42ac7ac27ab0b26ac9e0a2484cb53a71110001525c154459aec0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
