{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The classifier was developed with the nltk package, using its own classifier nltk.NaiveBayesClassifier() with a simple pipeline, that resembles word2vec:\n",
    "-  Acquisition of data\n",
    "-  Cleaning and pre-processing:\n",
    "    -  Removal of non-alphanumeric characters and words \n",
    "    -  Removal of stop-words of all the languages\n",
    "-  Tokenization \n",
    "-  Creation of the Bag of Words \n",
    "-  Splitting training and testing sets \n",
    "-  Training the model \n",
    "-  Testing and Querying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.classify import accuracy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import udhr\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Acquisition - Creating corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus was made mixing 9 pre-existing nltk corpus, from the Genesis corpus and the Universal declaration of human rights corpus:\n",
    "-  3 of the corpus are in english (two from the genesis and one from the Udhr)\n",
    "-  The other languages used are: Finnish, French (2 corpus), Portuguese, German and Spanish "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = { 'corpus':\n",
    "        [genesis.words(\"english-kjv.txt\"),genesis.words(\"english-web.txt\"),\n",
    "            genesis.words(\"finnish.txt\"),genesis.words(\"french.txt\"),genesis.words(\"portuguese.txt\"),\n",
    "            gutenberg.words(\"austen-emma.txt\"),gutenberg.words(\"shakespeare-macbeth.txt\"),\n",
    "            udhr.words(\"English-Latin1\"),udhr.words(\"German_Deutsch-Latin1\"),\n",
    "            udhr.words(\"French_Francais-Latin1\"),udhr.words(\"Spanish-Latin1\")\n",
    "        ],\n",
    "        'language':\n",
    "        [1,1,0,0,0,1,1,1,0,0,0]\n",
    "}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information about the corpus.\n",
      "(Where 1 is english and 0 is non-english)\n",
      "\n",
      "Information about corpus number 0\n",
      " Length: 44764, Lexical diversity: 0.06230453042623537, Language: 1\n",
      "Information about corpus number 1\n",
      " Length: 44054, Lexical diversity: 0.06033504335588142, Language: 1\n",
      "Information about corpus number 2\n",
      " Length: 32520, Lexical diversity: 0.2088560885608856, Language: 0\n",
      "Information about corpus number 3\n",
      " Length: 46116, Lexical diversity: 0.0803842484170353, Language: 0\n",
      "Information about corpus number 4\n",
      " Length: 45094, Lexical diversity: 0.08457887967357076, Language: 0\n",
      "Information about corpus number 5\n",
      " Length: 192427, Lexical diversity: 0.04059201671283136, Language: 1\n",
      "Information about corpus number 6\n",
      " Length: 23140, Lexical diversity: 0.17359550561797754, Language: 1\n",
      "Information about corpus number 7\n",
      " Length: 1781, Lexical diversity: 0.29927007299270075, Language: 1\n",
      "Information about corpus number 8\n",
      " Length: 1521, Lexical diversity: 0.3806706114398422, Language: 0\n",
      "Information about corpus number 9\n",
      " Length: 1935, Lexical diversity: 0.2930232558139535, Language: 0\n",
      "Information about corpus number 10\n",
      " Length: 1763, Lexical diversity: 0.3074305161656268, Language: 0\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "\n",
    "print('\\nInformation about the corpus.\\n(Where 1 is english and 0 is non-english)\\n')\n",
    "\n",
    "for index,corpus in df.iterrows():\n",
    "    print(f\"Information about corpus number {index}\\n Length: {len(corpus['corpus'])}, Lexical diversity: {lexical_diversity(corpus['corpus'])}, Language: {corpus['language']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning - Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonalpha(string):\n",
    "    return [word for word in string if re.match(r'[a-zA-Z]+',word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus']=df['corpus'].apply(lambda cw : remove_nonalpha(cw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string):\n",
    "    stop_words = set(stopwords.words('english') + stopwords.words('french') + \n",
    "        stopwords.words('finnish') + stopwords.words('portuguese') + \n",
    "        stopwords.words('german')+ stopwords.words('spanish'))\n",
    "    return ' '.join(word.lower() for word in string if word not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus']=df['corpus'].apply(lambda cw : remove_stopwords(cw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4/5. Tokenization and creation of the BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofwords(corpus):\n",
    "    w_tokenizer = WhitespaceTokenizer()\n",
    "    return dict(Counter(token for token in w_tokenizer.tokenize(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus'] = df['corpus'].apply(lambda cw : bagofwords(cw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating labeled corpus in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_corpusno = [\n",
    "    ({word:freq}, 'non-english') \n",
    "    for corp in df[df['language'] == 0]['corpus']\n",
    "    for word,freq in corp.items()\n",
    "]\n",
    "labeled_corpuseng = [\n",
    "    ({word:freq}, 'english') \n",
    "    for corp in df[df['language'] == 1]['corpus']\n",
    "    for word,freq in corp.items()\n",
    "    ]\n",
    "total_set = labeled_corpuseng + labeled_corpusno\n",
    "random.shuffle(total_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and testing set\n",
    "Common split percentages include:\n",
    "-  Train: 80%, Test: 20%\n",
    "-  Train: 67%, Test: 33%\n",
    "-  Train: 50%, Test: 50%\n",
    "Given the size of the corpus and some experiments during the developing of the code, it's been chosen to use the 67% and 33% split of the dataset.\\\n",
    "In the end, after the preprocessing and cleaning of the data, the size of the train set is 19574 words and the size of the test set is 10514 words.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = total_set[math.ceil(2*(len(labeled_corpuseng)/3)):] \n",
    "test = total_set[:math.ceil(2*(len(labeled_corpuseng)/3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 19574\n",
      "Size of test set: 10514\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of train set: {len(train)}\\nSize of test set: {len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    baal = 2              non-en : englis =      2.6 : 1.0\n",
      "                   hadar = 1              non-en : englis =      2.6 : 1.0\n",
      "                   jubal = 1              non-en : englis =      2.6 : 1.0\n",
      "                 magdiel = 1              non-en : englis =      2.6 : 1.0\n",
      "                  merari = 1              non-en : englis =      2.6 : 1.0\n",
      "                   abida = 1              non-en : englis =      1.8 : 1.0\n",
      "                   arodi = 1              non-en : englis =      1.8 : 1.0\n",
      "                   hamor = 11             non-en : englis =      1.8 : 1.0\n",
      "                   jabal = 1              non-en : englis =      1.8 : 1.0\n",
      "                  jemuel = 1              non-en : englis =      1.8 : 1.0\n",
      "                   ludim = 1              non-en : englis =      1.8 : 1.0\n",
      "                   magog = 1              non-en : englis =      1.8 : 1.0\n",
      "                    moab = 2              non-en : englis =      1.8 : 1.0\n",
      "                  nimrod = 2              non-en : englis =      1.8 : 1.0\n",
      "                    ohad = 1              non-en : englis =      1.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Testing and Querying the model\n",
    "\n",
    "## Performance indicators\n",
    "The performance of the classifier on the constructed test set is analyzed with confusion matrix by measuring the standard\n",
    "metrics that are commonly used for measuring the classification performance of other classification models.\\\n",
    "The experiments are evaluated using the standard metrics of accuracy, precision, recall and F-measure for classification.\n",
    "These were calculated using the predictive classification table, known as Confusion Matrix, where:\\\n",
    "- TN (True Negative) : Number of correct predictions that an instance is irrelevant\n",
    "- FP (False Positive) : Number of incorrect predictions that an instance is relevant\n",
    "- FN (False Negative) : Number of incorrect predictions that an instance is irrelevant\n",
    "- TP (True Positive) : Number of correct predictions that an instance is relevant\n",
    "- Accuracy(ACC) - The proportion of the total number of predictions that were correct:\n",
    "    - Accuracy (\\%) = (TN + TP)/(TN+FN+FP+TP)\n",
    "- Precision(PREC) - The proportion of the predicted relevant materials data sets that were correct:\n",
    "    - Precision (\\%) = TP / (FP + TP)\n",
    "- Recall(REC) - The proportion of the relevant materials data sets that were correctly identified\n",
    "    - Recall (\\%) = TP / (FN + TP)\n",
    "- F-Measure(FM) - Derives from precision and recall values:\n",
    "    - F-Measure (\\%) = (2 x REC x PREC)/(REC + PREC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5246338215712384\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wolabes = [item[0] for item in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classified = classifier.classify_many(test_wolabes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [elem[1]\n",
    "    for elem in test \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(reference,test_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            |         n |\n",
      "            |         o |\n",
      "            |         n |\n",
      "            |         - |\n",
      "            |    e    e |\n",
      "            |    n    n |\n",
      "            |    g    g |\n",
      "            |    l    l |\n",
      "            |    i    i |\n",
      "            |    s    s |\n",
      "            |    h    h |\n",
      "------------+-----------+\n",
      "    english |<5455>  65 |\n",
      "non-english | 4933  <61>|\n",
      "------------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Tag | Prec.  | Recall | F-measure\n",
      "------------+--------+--------+-----------\n",
      "    english | 0.5251 | 0.9882 | 0.6858\n",
      "non-english | 0.4841 | 0.0122 | 0.0238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cm.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employability\n",
    "Each row as to be intended as the one in which the label is the True statement (so we'll look at the first one). Given the F-measure though I don't think it is actually employable as classifier.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp-lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c64eeec08d42ac7ac27ab0b26ac9e0a2484cb53a71110001525c154459aec0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
