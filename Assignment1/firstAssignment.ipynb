{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The classifier was developed with the nltk package, using its own classifier nltk.NaiveBayesClassifier() with a simple pipeline, that resembles word2vec:\n",
    "-  Acquisition of data\n",
    "-  Cleaning and pre-processing:\n",
    "    -  Removal of non-alphanumeric characters and words \n",
    "    -  Removal of stop-words of all the languages\n",
    "-  Tokenization \n",
    "-  Creation of the Bag of Words \n",
    "-  Splitting training and testing sets \n",
    "-  Training the model \n",
    "-  Testing and Querying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Acquisition - Creating corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus was made mixing 9 pre-existing nltk corpus, from the Genesis corpus and the Universal declaration of human rights corpus:\n",
    "-  3 of the corpus are in english (two from the genesis and one from the Udhr)\n",
    "-  The other languages used are: Finnish, French (2 corpus), Portuguese, German and Spanish "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = genesis.words(\"english-kjv.txt\")\n",
    "english_web = genesis.words(\"english-web.txt\")\n",
    "finnish = genesis.words(\"finnish.txt\")\n",
    "french = genesis.words(\"french.txt\")\n",
    "portuguese = genesis.words(\"portuguese.txt\")\n",
    "\n",
    "languages = ['English-Latin1', 'German_Deutsch-Latin1', 'French_Francais-Latin1', 'Spanish-Latin1']\n",
    "\n",
    "d = {'genesis_corpus': [english, english_web, french, finnish,portuguese], 'language': [1,1, 0,0,0]}\n",
    "for language in languages:\n",
    "    if language != 'English-Latin1':\n",
    "        d['genesis_corpus'].append(udhr.words(language))\n",
    "        d['language'].append(0)\n",
    "    else:\n",
    "        d['genesis_corpus'].append(udhr.words(language))\n",
    "        d['language'].append(1)\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about genesis corpus in english.\n",
      " Length: 44764, Lexical diversity: 0.06230453042623537\n",
      "Information about genesis corpus in english (web).\n",
      " Length: 44054, Lexical diversity: 0.06033504335588142\n",
      "Information about genesis corpus in finnish.\n",
      " Length: 32520, Lexical diversity: 0.2088560885608856\n",
      "Information about genesis corpus in french.\n",
      " Length: 46116, Lexical diversity: 0.0803842484170353\n",
      "Information about genesis corpus in portuguese.\n",
      " Length: 45094, Lexical diversity: 0.08457887967357076\n",
      "Information about udhr corpus in english-latin1.\n",
      " Length: 1781, Lexical diversity: 0.29927007299270075\n",
      "Information about udhr corpus in german.\n",
      " Length: 1521, Lexical diversity: 0.3806706114398422\n",
      "Information about udhr corpus in french.\n",
      " Length: 1935, Lexical diversity: 0.2930232558139535\n",
      "Information about udhr corpus in spanish.\n",
      " Length: 1763, Lexical diversity: 0.3074305161656268\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "\n",
    "print(f'Information about genesis corpus in english.\\n Length: {len(english)}, Lexical diversity: {lexical_diversity(english)}')\n",
    "print(f'Information about genesis corpus in english (web).\\n Length: {len(english_web)}, Lexical diversity: {lexical_diversity(english_web)}')\n",
    "print(f'Information about genesis corpus in finnish.\\n Length: {len(finnish)}, Lexical diversity: {lexical_diversity(finnish)}')\n",
    "print(f'Information about genesis corpus in french.\\n Length: {len(french)}, Lexical diversity: {lexical_diversity(french)}')\n",
    "print(f'Information about genesis corpus in portuguese.\\n Length: {len(portuguese)}, Lexical diversity: {lexical_diversity(portuguese)}')\n",
    "print(f\"Information about udhr corpus in english-latin1.\\n Length: {len(d['genesis_corpus'][5])}, Lexical diversity: {lexical_diversity(d['genesis_corpus'][5])}\")\n",
    "print(f\"Information about udhr corpus in german.\\n Length: {len(d['genesis_corpus'][6])}, Lexical diversity: {lexical_diversity(d['genesis_corpus'][6])}\")\n",
    "print(f\"Information about udhr corpus in french.\\n Length: {len(d['genesis_corpus'][7])}, Lexical diversity: {lexical_diversity(d['genesis_corpus'][7])}\")\n",
    "print(f\"Information about udhr corpus in spanish.\\n Length: {len(d['genesis_corpus'][8])}, Lexical diversity: {lexical_diversity(d['genesis_corpus'][8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning - Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonalpha(string):\n",
    "    results = [word for word in string if re.match(r'[a-zA-Z]+',word)]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genesis_corpus']=df['genesis_corpus'].apply(lambda cw : remove_nonalpha(cw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string):\n",
    "    stop_words = set(stopwords.words('english') + stopwords.words('french') + \n",
    "    stopwords.words('finnish') + stopwords.words('portuguese') + \n",
    "    stopwords.words('german')+ stopwords.words('spanish'))\n",
    "    results = []\n",
    "    for word in string:\n",
    "        if word not in stop_words: results.append(word.lower())\n",
    "    return ' '.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genesis_corpus']=df['genesis_corpus'].apply(lambda cw : remove_stopwords(cw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4/5. Tokenization and creation of the BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "#lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofwords(corpus):\n",
    "    wordfreq = {}\n",
    "    tokens = w_tokenizer.tokenize(corpus)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "    return wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genesis_corpus'] = df['genesis_corpus'].apply(lambda cw : bagofwords(cw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating labeled corpus in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_eng = list(np.where(df['language'] == 0)[0])\n",
    "eng = list(np.where(df['language'] == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_corpusno = [\n",
    "    ({word:freq}, 'non-english') \n",
    "    for corp in df.iloc[no_eng]['genesis_corpus']\n",
    "    for word,freq in corp.items()\n",
    "]\n",
    "labeled_corpuseng = [\n",
    "    ({word:freq}, 'english') \n",
    "    for corp in df.iloc[eng]['genesis_corpus']\n",
    "    for word,freq in corp.items()\n",
    "    ]\n",
    "featureset = labeled_corpuseng + labeled_corpusno\n",
    "random.shuffle(featureset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = featureset[:math.ceil(2*(len(featureset)/3))]\n",
    "test = featureset[math.ceil(2*(len(featureset)/3)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.NaiveBayesClassifier.trainclassifier = (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 abimael = 1              englis : non-en =      4.5 : 1.0\n",
      "                   altar = 13             englis : non-en =      4.5 : 1.0\n",
      "                   alvan = 1              englis : non-en =      4.5 : 1.0\n",
      "                 archers = 1              englis : non-en =      4.5 : 1.0\n",
      "                   bedad = 1              englis : non-en =      4.5 : 1.0\n",
      "              concubines = 1              englis : non-en =      4.5 : 1.0\n",
      "                 dodanim = 1              englis : non-en =      4.5 : 1.0\n",
      "                    ebal = 1              englis : non-en =      4.5 : 1.0\n",
      "                 ellasar = 2              englis : non-en =      4.5 : 1.0\n",
      "                    gaza = 1              englis : non-en =      4.5 : 1.0\n",
      "                  gopher = 1              englis : non-en =      4.5 : 1.0\n",
      "                   hamul = 1              englis : non-en =      4.5 : 1.0\n",
      "                   heber = 1              englis : non-en =      4.5 : 1.0\n",
      "                  hemdan = 1              englis : non-en =      4.5 : 1.0\n",
      "                     hul = 1              englis : non-en =      4.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Testing and Querying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7861942577886377\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wolabes = [item[0] for item in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classified = classifier.classify_many(test_wolabes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [elem[1]\n",
    "    for elem in test \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = ConfusionMatrix(reference,test_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            |         n |\n",
      "            |         o |\n",
      "            |         n |\n",
      "            |         - |\n",
      "            |    e    e |\n",
      "            |    n    n |\n",
      "            |    g    g |\n",
      "            |    l    l |\n",
      "            |    i    i |\n",
      "            |    s    s |\n",
      "            |    h    h |\n",
      "------------+-----------+\n",
      "    english | <458>1325 |\n",
      "non-english |   75<4690>|\n",
      "------------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8592870544090057"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.precision('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2568704430734717"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.recall('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7797173732335827"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.precision('non-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9842602308499475"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.recall('non-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3955094991364422"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.f_measure('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8701298701298701"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.f_measure('non-english')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp-lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c64eeec08d42ac7ac27ab0b26ac9e0a2484cb53a71110001525c154459aec0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
